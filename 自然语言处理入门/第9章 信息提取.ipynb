{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新词提取（词典之外的词语）\n",
    "无监督算法提取新词\n",
    "1. 提取大量文本中的词语，无论新旧\n",
    "2. 利用词典过滤掉已有的词语，于是得到新词\n",
    "\n",
    "### 信息熵\n",
    "信息熵 - 指某条信息含有的信息量。它反映的是听说某个消息之后，关于该事件不确定性的减少量。熵越大不确定性越高。\n",
    "\n",
    "具体的在提取新词的过程中，给定字符串S作为词语备选，X定义为该字符串左边可能出现的字符，则H(X)称为左信息熵，类似的，定义右信息熵H(Y)\n",
    "\n",
    "### 互信息\n",
    "互信息 - 两个离散随机变量X与Y的相关程度。\n",
    "\n",
    "### 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_utility.py\n",
    "def test_data_path():\n",
    "    \"\"\"获取测试数据路径\"\"\"\n",
    "    data_path = os.path.join(HANLP_DATA_PATH, 'test')\n",
    "    if not os.path.isdir(data_path):\n",
    "        os.mkdir(data_path)\n",
    "    return data_path\n",
    "\n",
    "\n",
    "def ensure_data(data_name, data_url: str):\n",
    "    \"\"\"目标文件下载模块\"\"\"\n",
    "    root_path = test_data_path()\n",
    "    dest_path = os.path.join(root_path, data_name)\n",
    "    # 如果dest_path文件存在,直接返回文件路径\n",
    "    if os.path.exists(dest_path):\n",
    "        return dest_path\n",
    "    \n",
    "    if data_url.endswith('.zip'):\n",
    "        dest_path += '.zip'\n",
    "    # 文件不存在时 下载数据到指定目录\n",
    "    download(data_url, dest_path)\n",
    "    # 解压文件\n",
    "    if data_url.endswith('.zip'):\n",
    "        with zipfile.ZipFile(dest_path, 'r') as archive:\n",
    "            archive.extractall(root_path)\n",
    "        remove_file(dest_path)   # 删除压缩包\n",
    "        dest_path = dest_path[:-4]\n",
    "    return dest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "from pyhanlp import *\n",
    "from pyhanlp.static import download, remove_file, HANLP_DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "下载 http://file.hankcs.com/corpus/红楼梦.zip 到 /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pyhanlp/static/data/test/红楼梦.txt.zip\n",
      "100.00%, 1 MB, 273 KB/s, 还有 0 分  0 秒   \n"
     ]
    }
   ],
   "source": [
    "HLM_PATH = ensure_data(\"红楼梦.txt\", \"http://file.hankcs.com/corpus/红楼梦.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(corpus):\n",
    "    # print(\"%s 热词\" % corpus)\n",
    "    # word_info_list = HanLP.extractWords(IOUtil.newBufferedReader(corpus), 100)\n",
    "    # 参数，以此为\n",
    "    # reader: 数据源或巨大的字符串\n",
    "    # size: 控制返回多少个词语\n",
    "    # newWordsOnly: 为真的时候，使用内部词典过滤掉旧词，只返回OOV\n",
    "    # max_word_len: 识别结果中最长的词语长度，默认值为4\n",
    "    # min_freq: 控制结果中词语的最低频率，低于该频率将会被过滤掉\n",
    "    # min_entropy: 控制结果中词语的最低信息熵，一般取0.5，该之越大越短的词语越容易被取出来\n",
    "    # min_aggregation: 结果中词语最低互信息熵的值，一般取50-200，越大，越长的词语越容易被提取出来\n",
    "    # print(word_info_list)\n",
    "    print(\"%s 新词\" % corpus)\n",
    "    word_info_list = HanLP.extractWords(\n",
    "        IOUtil.newBufferedReader(corpus), 200, True,\n",
    "        min_aggregation=300\n",
    "    )\n",
    "    print(word_info_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pyhanlp/static/data/test/红楼梦.txt 新词\n",
      "[薛姨妈, 贾珍, 刘姥姥, 麝月, 贾蓉, 周瑞, 贾赦, 雨村, 贾芸, 芳官, 贾环, 林姑娘, 赵姨娘, 莺儿, 两银子, 宝蟾, 溃骸, 秦钟, 薛蝌, 几句, 岫烟, 赖大, 茗烟, 听了这话, 递与, 钏儿, 士隐, 荣府, 贾蔷, 冯紫英, 焙茗, 请了安, 宁府, 金钏, 包勇, 代儒, 鲍二, 嗳哟, 在床上, 从小儿, 十六, 既这么, 让坐, 李贵, 打谅, 既这样, 金钏儿, 日一早, 李嬷嬷, 唬了一跳, 族中, 还了得, 王仁, 蘅芜, 在炕上, 藕官, 间屋, 尤三姐, 五百, 几句话, 警幻, 也未可知, 手帕子, 忽然想起, 越性, 十两银子, 李婶, 熙凤, 琏二奶奶, 十九, 珍大爷, 六十, 小蹄子, 蒋玉菡, 要知端, 往那里去, 榻上, 些闲话, 孙女儿, 罢咧, Φ溃骸, 八九, 乱嚷, 自己房中, 犹未, 既如此, 点点头儿, 李十儿, 几个钱, 在外间, 沁芳, 忙陪笑, 千万, 十七, 几两银子, 王子腾, 李纹, 铁槛寺, 马道婆, 张道士, 七八, 宁荣, 若论, 素习, 九十, 担骸, 贾赦贾政, 顽耍, 形景, 王太医, 至房中, 龄官, 蕊官, 梨香院, 怡红院中, 柳湘莲, チ耍, 回到自己, 栊翠庵, 贾母这边, 还不快, 俱已, 啐了一口, 归坐, 见他这般, 邢岫烟, 醴蛉, 请大夫, 到自己房, 手里拿, 百银子, 娘儿两个, 掉下, 李绮, 成日家, 佩凤, 溃骸澳, 点头叹, 也不答言, 小太监, 请医, 五六, 蘅芜苑, 换了衣, 绣桔, 瞧了一瞧, 詹光, 邢大舅, 回至房中, 要紧的事, 令其, 没了主意, 又气又, 懒待, 外书房, 咸, 舅太爷, 应了一声, 打听打听, 什么大事, 百两银子, 亲家太太, 李宫裁, 老子娘, 分例, 焦大, 碗茶, 之态, 忽又想, 忘八, 往贾母处, 于是大家, 头不语, 堂官, 当铺里, 王一贴, 据我看, 皆系, 第二件, 在炕沿, 见了这般, 溃骸拔, 手里拿着, 起帘子, 磕了头, 方才的话, 椅子上, 花自芳, 档溃骸, 兰哥, 五十两, 荣宁, 北τ, 世职, 李婶娘, 坐了一坐, 坐一坐, 吊钱, 三个字, 连忙答应]\n"
     ]
    }
   ],
   "source": [
    "extract(HLM_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关键词提取\n",
    "无监督关键词提取算法：词频、TF-IDF和TextRank；单文档算法能够独立分析每篇文章的关键词，包括词频和TextRank；而多文档算法利用其他文档来辅助决定关键词，即TF-IDF。\n",
    "\n",
    "### 词频统计\n",
    "通过统计文档中反复出现的词语并排序，但是要注意去除不重要的、意义不大的词语，在统计词频前要过滤掉这些词语。词频统计的一般流程是分词、停用词过滤、按词频提取前n个\n",
    "\n",
    "### TF-IDF\n",
    "相较于词频，TF-IDF还综合考虑词语的稀有程度。在TF-IDF计算方法中，一个词的重要程度不仅要考虑他在文档中的频次，还要反比于有多少文档包含它。包含该词语的文档越多，说明它越宽泛，越不能体现文档特色。\n",
    "$$TF-IDF(t,d) = \\frac{TF(t,d)}{DF(t)}=TF(t,d) * IDF(t)$$\n",
    "- TF(t,d)表示单词t在文档d中出现的频次；\n",
    "- DF(t) 表示有多少文档包含单词t;DF 的倒数称为IDF。\n",
    "\n",
    "### TextRank\n",
    "TextRank是PageRank 在文本上的应用，反映了单词的重要性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhanlp import *\n",
    "\n",
    "\n",
    "def demo_keyword(content):\n",
    "    \"\"\" 关键词提取\n",
    "    >>> content = (\n",
    "    ...    \"程序员(英文Programmer)是从事程序开发、维护的专业人员。\"\n",
    "    ...    \"一般将程序员分为程序设计人员和程序编码人员，\"\n",
    "    ...    \"但两者的界限并不非常清楚，特别是在中国。\"\n",
    "    ...    \"软件从业人员分为初级程序员、高级程序员、系统\"\n",
    "    ...    \"分析员和项目经理四大类。\")\n",
    "    >>> demo_keyword(content)\n",
    "    [程序员, 程序, 分为, 人员, 软件]\n",
    "    \"\"\"\n",
    "    TextRankKeyword = JClass(\"com.hankcs.hanlp.summary.TextRankKeyword\")\n",
    "    keyword_list = HanLP.extractKeyword(content, 10)\n",
    "    print(keyword_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[袜子, 卖, 臭, 寝室, 干, 长得, 穿过, 大学, 室友, 兴奋]\n"
     ]
    }
   ],
   "source": [
    "content = (\n",
    "    \"我大学室友，长得白白净净，很甜美，算是美少女类型的。\"\n",
    "    \"那天，上课上一半肚子疼，我就提前回寝室了。\"\n",
    "    \"一开门，发现室友对着自己的臭袜子在拍照片。\"\n",
    "    \"她那袜子是真的臭，全寝室出了名的。当时我就被恶心到了，随口说了句，干啥呢？\"\n",
    "    \"她不好意思笑了笑，有人想看看货。原来她常年在网上卖自己穿过的袜子，难怪从来不看她洗。\"\n",
    "    \"我看了她的收款记录，一双穿过的臭袜子，竟卖到100块钱一双。她还告诉我，长得越漂亮，袜子的价格卖得越贵。\"\n",
    "    \"她鼓动我也卖袜子，就我这长相，100多一双没问题。上过大学的都知道，大学生穷啊，这个生意还是吸引了我。\"\n",
    "    \"于是我尝试性地发布了一条袜子，最终150成交。当天晚上我就兴奋得没睡着，因为我要干一番大事业！。\"\n",
    ")\n",
    "demo_keyword(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 短语提取\n",
    "常用于搜索引擎的自动推荐，文档简介的生成等。利用互信息和左右信息熵可以将新词提取算法扩展到短语提取。只需要将字替换为单词即可。\n",
    "\n",
    "### 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import doctest\n",
    "from pyhanlp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "先记住两个概念：峰值和终值。峰值和终值，是由2002诺贝尔奖得主、心理学家丹尼尔·卡尼曼提出的。\n",
    "他发现大家对体验的记忆由两个核心因素决定：第一个是体验最高峰的时候，无论是正向的最高峰还是负向的最高峰，一定是能记得住的。\n",
    "（峰值）第二个是结束时的感觉。（终值）这就是峰终定律（Peak-End Rule）。（转引自梁宁《产品思维30讲》）\n",
    "/ 02 /说到这里，你可以回想一下自己平时在学习时的体验：过程中基本上很少有正向的高峰，\n",
    "总是被“我不想做啊”、“这个好麻烦啊”、“太枯燥了”、“我想出去玩”、“我不会”这些负面情绪充斥。\n",
    "你每次学习的峰值体验，基本上都是负向的。关于“终值”，每次你结束学习时，一般都是很狼狈的吧？\n",
    "实在是被自己打败了，实在是学不进去了，烦躁到极致，赶紧像个逃兵一样收拾书包滚蛋了。\n",
    "正是因为，每次你的学习过程，都充满着这样负向的峰值和终值，而峰终值，直接决定了你对“学习”的所有记忆。\n",
    "所以一旦你想起学习，内心满是厌恶、排斥，甚至恐惧。你每次做这件事时，感受都是非常不愉悦，怎么可能会有“我想做”的想法呢？\n",
    "当然一点都不会有啊，谁没事儿喜欢给自己找罪受啊，又不是抖M。/ 03 /好辣，两个概念你记好；你为什么厌恶、排斥、害怕学习，\n",
    "我们也在这两个概念的基础上，大致分析清楚了。接下来，冷冷老师教你解决问题，让你在一定程度上喜欢学习，主动有“我想学”的念头。\n",
    "体验一个事物或产品之后，所能记住的就只有在峰值与终值时的体验，而整个过程中每个点好与不好，时间长短，对记忆或者感受都没那么大影响。\n",
    "（梁宁《产品思维30讲》）嚯，是不是发现可乘之机了？hhhh。影响因素只有峰终值，过程中其他繁多的点，影响不大。我们可以利用这个基本原理，\n",
    "在学习过程中，主动去创造正向的峰值和终值。我们还是说点能听得懂的人话吧。下文是两颗栗子。 / 04 /虽然学习在总体上是枯燥的，\n",
    "但总有你喜欢的某个科目；某个科目，总有你喜欢且擅长的某个章节；或者，总有某个环节是你喜欢的：喜欢看书？或者是喜欢做题？也可能喜欢背单词？\n",
    "每次的学习过程，穿插着安排你喜欢且擅长的科目/章节/环节。如此，你每次都会有学得得心应手、轻松愉悦、反馈满满的时刻，也就是出现了正向的“峰值”。\n",
    "每当你学习难搞、恐惧的科目，在负面情绪达到顶峰之前，可以先转换到喜欢且擅长的环节，情绪安抚后，再回头继续做难的事情。以此避免负向峰值的出现。\n",
    "比如我写文章时，需要做的两件主要的事情，是“输入”和“输出”。我非常喜欢“输入”：看书、看公开课、跟人探讨、学习一些付费课程等等，无论多枯燥多抽象，\n",
    "都觉得很有意思。“输出”比输入难多了，写得不顺畅，就会很烦躁。所以我每次只写25分钟，时间一到，虽然没完成，也立刻收手，去做我喜欢的“输入”。\n",
    "这一过程来回替换。整个过程基本不会出现负向的“峰值”；“输入”和“输出”，都会及时高效完成\n",
    "。/ 05 / 关于终值，每次学习的末尾，都要以你最喜欢、最能带给你反馈感的环节结束。别去学最难的东西，也别在纠结焦虑中，\n",
    "让学习体验以痛苦终结。打颗栗子，我平时写公众号文章，有时选题熟悉，写得顺畅，过程很愉悦。但如果当天标题起得不好，阅读量只有平时一半。\n",
    "我的“终值”体验，是比较糟糕的；之前所有顺畅愉悦的体验，我会尽数忘记，只记得糟糕的“终值”，以至于下次写文时，常有提不起劲、排斥的感觉。\n",
    "也有好些时候，选题比较复杂繁琐，写的过程很费精力，后面起标题时，苦思冥想一个小时，最后战战兢兢推送。好家伙，阅读量很好。\n",
    "这个“终值”，立刻覆盖了之前写作和起标题过程中难以为继的各种痛苦体验。下次写文时，我会隐隐期待，有强烈的“我想写”的感觉。\n",
    "看吧，“终值”体验就是这么强大，甚至连峰值体验都能覆盖掉。如果运用在学习过程中，是不是一个道理？\n",
    "在你反馈感很强、情绪高涨满足的时刻结束学习，留下一个正向的终值。千万别学到想吐才结束。\n",
    "/ 06 /好辣，记住了么？峰值：体验最高峰，包括正向和负向。终值：结束时的感觉。二者共同决定我们对某一事件的体验和记忆。\n",
    "无论是学习、工作、跑步、读书，还是与男/女朋友约会，或者任何事情！都可以巧妙利用这一定律，创造正向的峰值和终值体验，会让你对整件事情的记忆，\n",
    "变得愉悦起来。爱学认学，从峰终值开始。大脑可真好骗吼，像个大傻子。0.0感谢阅读。码字不易，费时费心，如果文章对你有帮助，还请花1秒钟点个赞。：）\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[过程中, 喜欢擅长, 负向峰值, 每次学习, 峰值终值]\n"
     ]
    }
   ],
   "source": [
    "phrase_list = HanLP.extractPhrase(text, 5)\n",
    "print(phrase_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关键句提取\n",
    "关键词或短语过于碎片化，无法完整的表达主题，这就需要关键句子的提取，关键句的提取是基于PageRank的扩展。\n",
    "引入BM25算法衡量句子相似度，改进链接的权重。\n",
    "\n",
    "### BM25\n",
    "BM25算法是TF-IDF的一种改进变种。TF-IDF衡量的是单个词语在文档中的重要程度，而在搜索领域中，查询串是有多个词语构成的，衡量多个词语与文档的关联程度，就是BM25解决的问题\n",
    "\n",
    "### TextRank\n",
    "将句子作为查询语句，将相邻句子视作待查询的文档，不再取窗口，认为所有的句子都是相邻的\n",
    "\n",
    "### 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[你每次学习的峰值体验, 创造正向的峰值和终值体验, 每次的学习过程, 每次你的学习过程, 所能记住的就只有在峰值与终值时的体验]\n"
     ]
    }
   ],
   "source": [
    "TextRankSentence = JClass(\"com.hankcs.hanlp.summary.TextRankSentence\")\n",
    "sentence_list = HanLP.extractSummary(text, 5)\n",
    "print(sentence_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "1. 新词提取：互信息和左右信息熵；\n",
    "     - 互信息：两个离散变量的关联程度，即单个字符串间的关联程度，互信息越大，被切分的可能越小\n",
    "     - 左右信息熵：字符串左右可能出现的字符串的信息熵，左右信息熵越大，说明该字符串的搭配越丰富，是一个词的可能性越大\n",
    "2. 关键词提取：TextRank、TF-IDF、词频统计\n",
    "    - 词频统计：一篇文档中词语出现频率较高的词语，除去无意义的停用词\n",
    "    - TF-IDF：tf--词语在一篇文档中的频率，idf--出现该词语的文档的数量的倒数。一个词在一篇文档中出现的次数多，出现在其他文档中的次数越少，该词越重要\n",
    "    - TextRank：基于pagerank演变而来，衡量一个词语的重要程度，中心词左右搭配越多，中心词越重要，同时中心词与另一个中心词搭配越多，其越不重要。\n",
    "3. 短语提取：互信息和左右信息熵\n",
    "4. 关键句提取：BM25和TextRank\n",
    "    - BM25：TF-IDF 的改进，用于衡量多个词语与文档的关联程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
