{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结：\n",
    "1. 二元语法模型：每次计算只涉及连续的两个单词的二元连续语言模型；\n",
    "2. 平滑策略：使语法模型频次折线平滑为曲线\n",
    "3. 词网： 一个句子中所有的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhanlp import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1加载语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[商品, 和, 服务]\n",
      "[商品, 和服, 物美价廉]\n",
      "[服务, 和, 货币]\n"
     ]
    }
   ],
   "source": [
    "# 语料加载器\n",
    "CorpusLoader = SafeJClass('com.hankcs.hanlp.corpus.document.CorpusLoader')\n",
    "# 自然语言标记器\n",
    "NatureDictionaryMaker = SafeJClass('com.hankcs.hanlp.corpus.dictionary.NatureDictionaryMaker')\n",
    "\n",
    "corpus_path = my_cws_corpus()\n",
    "sents = CorpusLoader.convert2SentenceList(corpus_path)\n",
    "for sent in sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2统计一元语法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bigram(corpus_path, model_path):\n",
    "    sents = CorpusLoader.convert2SentenceList(corpus_path)\n",
    "    for sent in sents:\n",
    "        for word in sent:\n",
    "            word.setLabel('n')\n",
    "    maker = NatureDictionaryMaker()\n",
    "    maker.compute(sents)\n",
    "    maker.saveTxtTo(model_path)\n",
    "    \n",
    "model_path = './data/my_cws_test/my_cws_model'\n",
    "train_bigram(corpus_path, model_path)\n",
    "\n",
    "# my_cws_model.ngram.txt 为自动生成的二元语法词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tests.test_utility import test_data_path\n",
    "# 实现test_data_path 方法\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "from pyhanlp.static import download, remove_file, HANLP_DATA_PATH\n",
    "\n",
    "\n",
    "def test_data_path():\n",
    "    \"\"\"\n",
    "    获取测试数据路径\n",
    "    \"\"\"\n",
    "    data_path = os.path.join(HANLP_DATA_PATH, 'test')\n",
    "    if not os.path.isdir(data_path):\n",
    "        os.mkdir(data_path)\n",
    "    return data_path\n",
    "\n",
    "\n",
    "# from test.test_utility import ensure_data\n",
    "def ensure_data(data_name, data_url):\n",
    "    root_path = test_data_path()\n",
    "    dest_path = os.path.join(root_path, data_name)\n",
    "    if os.path.exists(dest_path):\n",
    "        return dest_path\n",
    "    if data_url.endswith('.zip'):\n",
    "        dest_path += '.zip'\n",
    "    download(data_url, dest_path)\n",
    "    \n",
    "    # 解压文件，手动下载文件至目录中\n",
    "    if data_url.endswith('.zip'):\n",
    "        with zipfile.ZipFile(dest_path,'r') as archive:\n",
    "            archive.extractall(root_path)\n",
    "        remove_file(dest_path)\n",
    "        dest_path = dest_path[:len('.zip')]\n",
    "    return dest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tests.book.ch03.demo_corpus_loader import my_cws_corpus\n",
    "def my_cws_corpus():\n",
    "    \"\"\"\n",
    "    在指定文件目录中，创建测试语料库\n",
    "    \"\"\"\n",
    "    data_root = test_data_path()\n",
    "    corpus_path = os.path.join(data_root, 'my_cws_corpus.txt')\n",
    "    if not os.path.isfile(corpus_path):\n",
    "        with open(corpus_path, 'w', encoding='utf-8') as f:\n",
    "            f.write('商品 和 服务\\n商品 和服 物美价廉\\n服务 和 货币')\n",
    "    \n",
    "    return corpus_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 微软亚洲研究院语料库\n",
    "# from tests.book.ch03.msr import msr_model\n",
    "sighan05 = ensure_data('icwb2-data', 'http://sighan.cs.uchicago.edu/bakeoff2005/data/icwb2-data.zip')\n",
    "msr_dict = os.path.join(sighan05, 'gold', 'msr_training_words.utf8')\n",
    "msr_train = os.path.join(sighan05, 'training', 'msr_training.utf8')\n",
    "msr_model = os.path.join(test_data_path(), 'msr_cws')\n",
    "msr_test = os.path.join(sighan05, 'testing', 'msr_test.txt')\n",
    "msr_output = os.path.join(sighan05, 'testing', 'msr_bigram_output.txt')\n",
    "msr_gold = os.path.join(sighan05, 'gold', 'msr_test_gold.utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jpype import JString\n",
    "\n",
    "from pyhanlp import *\n",
    "# from tests.book.ch03.demo_corpus_loader import my_cws_corpus\n",
    "# from tests.book.ch03.msr import msr_model\n",
    "# from tests.test_utility import test_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 挂载接口\n",
    "NatureDictionaryMaker = SafeJClass('com.hankcs.hanlp.corpus.dictionary.NatureDictionaryMaker')\n",
    "CorpusLoader = SafeJClass('com.hankcs.hanlp.corpus.document.CorpusLoader')\n",
    "WordNet = JClass('com.hankcs.hanlp.seg.common.WordNet')\n",
    "Vertex = JClass('com.hankcs.hanlp.seg.common.Vertex')\n",
    "ViterbiSegment = JClass('com.hankcs.hanlp.seg.Viterbi.ViterbiSegment')\n",
    "DijksraSegment = JClass('com.hankcs.hanlp.seg.Dijkstra.DijkstraSegment')\n",
    "CoreDictionary = LazyLoadingJClass('com.hankcs.hanlp.dictionary.CoreDictionary')\n",
    "Nature = JClass('com.hankcs.hanlp.corpus.tag.Nature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bigram(corpus_path, model_path):\n",
    "    \"\"\"\n",
    "    训练二元网络模型\n",
    "    \"\"\"\n",
    "    # 转换为句子列表\n",
    "    sents = CorpusLoader.convert2SentenceList(corpus_path)\n",
    "    for sent in sents:\n",
    "        for word in sent:\n",
    "            if word.label is None:\n",
    "                word.setLabel('n')   # 重新打标签\n",
    "    maker = NatureDictionaryMaker()  # 生成标注词典\n",
    "    maker.compute(sents)\n",
    "    maker.saveTxtTo(model_path)\n",
    "    \n",
    "    \n",
    "def load_bigram(model_path, verbose=True, ret_viterbi=True):\n",
    "    # 核心词典路径\n",
    "    HanLP.Config.CoreDictionaryPath = model_path + '.txt'   # 一元模型\n",
    "    HanLP.Config.BiGramDictionaryPath = model_path + '.ngram.txt'\n",
    "    \n",
    "    if verbose:\n",
    "        # 获取某个词语出现的频率\n",
    "        print(CoreDictionary.getTermFrequency('商品'))\n",
    "        # print(CoreB)\n",
    "        sent = '商品和服务'\n",
    "        wordnet = generate_wordnet(sent, CoreDictionary.trie)\n",
    "        print(wordnet)\n",
    "        print(viterbi(wordnet))\n",
    "    return ViterbiSegment().enableAllNamedEntityRecognize(False).enableCustomDictionary(\n",
    "        False) if ret_viterbi else DijkstraSegment().enableAllNamedEntityRecognize(False).enableCustomDictionary(False)\n",
    "\n",
    "\n",
    "def generate_wordnet(sent, trie):\n",
    "    \"\"\"\n",
    "    生成词网\n",
    "    :param sent: 句子\n",
    "    :param trie: 词典\n",
    "    :return: 词网\n",
    "    \"\"\"\n",
    "    searcher = trie.getSearcher(JString(sent), 0)\n",
    "    wordnet = WordNet(sent)\n",
    "    while searcher.next():\n",
    "        wordnet.add(\n",
    "            searcher.begin + 1,\n",
    "            Vertex(\n",
    "                sent[searcher.begin:searcher.begin + searcher.length],\n",
    "                searcher.value,\n",
    "                searcher.index\n",
    "            )\n",
    "        )\n",
    "    vertexes = wordnet.getVertexes()\n",
    "    i = 0\n",
    "    while i < len(vertexes):\n",
    "        if len(vertexes[i]) == 0:   # 空行\n",
    "            j = i + 1\n",
    "            # 寻找第一个非空行\n",
    "            for j in range(i + 1, len(vertexes) - 1):\n",
    "                if len(vertexes[j]):\n",
    "                    break\n",
    "            wordnet.add(i, Vertex.newPunctuationInstance(sent[i - 1: j - 1]))\n",
    "            i = j\n",
    "        else:\n",
    "            i += len(vertexes[i][-1].realWord)\n",
    "    return wordnet\n",
    "\n",
    "\n",
    "def viterbi(wordnet):\n",
    "    nodes = wordnet.getVertexes()\n",
    "    for i in range(0, len(nodes) - 1):\n",
    "        for node in nodes[i]:\n",
    "            for to in nodes[i + len(node.realWord)]:\n",
    "                to.updateFrom(node)\n",
    "                \n",
    "    path = []\n",
    "    f = nodes[len(nodes) - 1].getFirst()\n",
    "    while f:\n",
    "        path.insert(0, f)\n",
    "        f = f.getFrom()\n",
    "    return [v.realWord for v in path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2209\n",
      "0:[ ]\n",
      "1:[商, 商品]\n",
      "2:[品]\n",
      "3:[和, 和服]\n",
      "4:[服, 服务]\n",
      "5:[务]\n",
      "6:[ ]\n",
      "\n",
      "[' ', '商品', '和', '服务', ' ']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<jpype._jclass.com.hankcs.hanlp.seg.Viterbi.ViterbiSegment at 0x7faf6d043390>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_path = my_cws_corpus()\n",
    "model_path = os.path.join(test_data_path(), 'my_cws_model')\n",
    "train_bigram(corpus_path, model_path)\n",
    "load_bigram(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
