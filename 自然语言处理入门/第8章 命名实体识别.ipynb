{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概述\n",
    "命名实体-描述实体的词汇，具有以下共性：\n",
    "- 数量无穷。\n",
    "- 构词灵活。\n",
    "- 类别模糊。\n",
    "\n",
    "识别出文本中命名实体的边界与类别的任务称为命名实体识别。\n",
    "\n",
    "## 基于规则的命名实体识别\n",
    "### 基于规则的音译人名识别\n",
    "其逻辑如下：\n",
    "1. 若粗分结果中某词语的备选词性含有 nrf 则触发规则 2；\n",
    "2. 从该词语出发从左往右扫描，若遇到音译人名库中的词语，则合并\n",
    "\n",
    "### 基于规则的日本人名识别\n",
    "1. 文本中匹配日本人名的姓氏和名字，记作x和m\n",
    "2. 合并连续的xm为日本人名\n",
    "\n",
    "### 基于规则的数词英文识别\n",
    "\n",
    "## 基于层叠隐马尔可夫模型的角色标注框架\n",
    "### 基于角色标注的中国人名识别\n",
    "中国科学院计算技术研究所软件实验室张华平和刘群教授在《基于角色标注的中国人名自动识别研究》\n",
    "- 1. 姓氏 - B\n",
    "- 2. 双名的首字 - C\n",
    "- 3. 双名的末字 - D\n",
    "- 4. 单名 - E\n",
    "- 5. 前缀 - F\n",
    " ……\n",
    " \n",
    "### 基于角色标注的地名识别\n",
    "- 1. 地名的上文 - A\n",
    "- 2. 地名的下文 - B\n",
    "\n",
    "### 基于角色标注的机构名称\n",
    "- 1. 上文 - A\n",
    "- 2. 下文 - B\n",
    "\n",
    "## 基于序列标注的命名实体识别\n",
    "1. hmm \n",
    "2. crf\n",
    "3. perceptron\n",
    "\n",
    "## 实现借口\n",
    "### 基于隐马尔可夫模型序列标注的命名实体识别\n",
    "https://github.com/hankcs/pyhanlp/blob/master/tests/book/ch08/demo_hmm_ner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "from pyhanlp import *\n",
    "from pyhanlp.static import download, remove_file, HANLP_DATA_PATH\n",
    "# from tests.book.ch07 import pku # 如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_utility.py\n",
    "def test_data_path():\n",
    "    \"\"\"获取测试数据路径\"\"\"\n",
    "    data_path = os.path.join(HANLP_DATA_PATH, 'test')\n",
    "    if not os.path.isdir(data_path):\n",
    "        os.mkdir(data_path)\n",
    "    return data_path\n",
    "\n",
    "\n",
    "def ensure_data(data_name, data_url: str):\n",
    "    \"\"\"目标文件下载模块\"\"\"\n",
    "    root_path = test_data_path()\n",
    "    dest_path = os.path.join(root_path, data_name)\n",
    "    # 如果dest_path文件存在,直接返回文件路径\n",
    "    if os.path.exists(dest_path):\n",
    "        return dest_path\n",
    "    \n",
    "    if data_url.endswith('.zip'):\n",
    "        dest_path += '.zip'\n",
    "    # 文件不存在时 下载数据到指定目录\n",
    "    download(data_url, dest_path)\n",
    "    # 解压文件\n",
    "    if data_url.endswith('.zip'):\n",
    "        with zipfile.ZipFile(dest_path, 'r') as archive:\n",
    "            archive.extractall(root_path)\n",
    "        remove_file(dest_path)   # 删除压缩包\n",
    "        dest_path = dest_path[:-4]\n",
    "    return dest_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pku.py  # 加载训练语料的路径\n",
    "# 以后使用hmm分词时，训练语料库的类型与此相同即可\n",
    "# 本目录下的`199801.txt`在原版的基础上做了如下修改：\n",
    "# 1. 为了符合习惯，姓+名合并为姓名\n",
    "# 2. 格式升级为兼容2014版，复合词中括号后添加“/”\n",
    "# 3. 文本编码调整为UTF-8\n",
    "PKU98 = ensure_data('pku98', \"http://file.hankcs.com/corpus/pku98.zip\")\n",
    "PKU199801 = os.path.join(PKU98, '199801.txt')\n",
    "PKU199801_TRAIN = os.path.join(PKU98, '199801-train.txt')\n",
    "PKU199801_TEST = os.path.join(PKU98, '199801-test.txt')\n",
    "POS_MODEL = os.path.join(PKU98, 'pos.bin')\n",
    "NER_MODEL = os.path.join(PKU98, 'ner.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "HMMNERecognizer = JClass('com.hankcs.hanlp.model.hmm.HMMNERecognizer')\n",
    "AbstractLexicalAnalyzer = JClass(\n",
    "    'com.hankcs.hanlp.tokenizer.lexical.AbstractLexicalAnalyzer')\n",
    "PerceptronSegmenter = JClass('com.hankcs.hanlp.model.perceptron.PerceptronSegmenter')\n",
    "PerceptronPOSTagger = JClass('com.hankcs.hanlp.model.perceptron.PerceptronPOSTagger')\n",
    "Utility = JClass('com.hankcs.hanlp.model.perceptron.utility.Utility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(corpus):\n",
    "    recognizer = HMMNERecognizer()\n",
    "    recognizer.train(corpus)\n",
    "    return recognizer\n",
    "\n",
    "def test(recognizer):\n",
    "    word_array = [\"华南\", \"电力\", \"公司\"]  # 构造单词序列\n",
    "    pos_array = [\"ns\", \"n\", \"n\"]  # 构造词性序列\n",
    "    ner_array = recognizer.recognize(word_array, pos_array)\n",
    "    for word, tag, ner in zip(word_array, pos_array, ner_array):\n",
    "        print(f'{word}\\t{tag}\\t{ner}')\n",
    "    analyzer = AbstractLexicalAnalyzer(\n",
    "        PerceptronSegmenter(), PerceptronPOSTagger(), recognizer)\n",
    "    print(analyzer.analyze(\"华南电力公司董事长刘良栋和秘书章梦丹来到美国纽约现代艺术博物馆参观\"))\n",
    "    scores = Utility.evaluateNER(recognizer, PKU199801_TEST)\n",
    "    print(scores)\n",
    "    Utility.printNERScore(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "华南\tns\tB-nt\n",
      "电力\tn\tM-nt\n",
      "公司\tn\tE-nt\n",
      "[华南/ns 电力/n 公司/n]/nt 董事长/n 刘良栋/nr 和/c 秘书/n 章梦丹/nr 来到/v 美国纽约/ns 现代/ntc 艺术/n 博物馆/n 参观/v\n",
      "{avg.=[D@396a51ab, ns=[D@51081592, nt=[D@7f9a81e8}\n"
     ]
    }
   ],
   "source": [
    "recognizer = train(PKU199801_TRAIN)\n",
    "test(recognizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于感知机序列标注的命名实体识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
