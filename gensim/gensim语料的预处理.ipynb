{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建预料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save logging events\n",
    "import logging\n",
    "# logging.basicConfig(format='%(asctime)s:%(levlename)s:%(message)s', level=logging.INFO)\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件目录 \"C:\\Users\\86132\\AppData\\Local\\Temp\" 将被用于保存临时文件和语料库。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "TEMP_FOLDER = tempfile.gettempdir()     # 创建一个临时文件，使用完成后自动删除\n",
    "print('文件目录 \"%s\" 将被用于保存临时文件和语料库。' % TEMP_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim  import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Dictionary in module gensim.corpora.dictionary:\n",
      "\n",
      "class Dictionary(gensim.utils.SaveLoad, collections.abc.Mapping)\n",
      " |  Dictionary(documents=None, prune_at=2000000)\n",
      " |  \n",
      " |  Dictionary encapsulates the mapping between normalized words and their integer ids.\n",
      " |  \n",
      " |  Notable instance attributes:\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  token2id : dict of (str, int)\n",
      " |      token -> tokenId.\n",
      " |  id2token : dict of (int, str)\n",
      " |      Reverse mapping for token2id, initialized in a lazy manner to save memory (not created until needed).\n",
      " |  cfs : dict of (int, int)\n",
      " |      Collection frequencies: token_id -> how many instances of this token are contained in the documents.\n",
      " |  dfs : dict of (int, int)\n",
      " |      Document frequencies: token_id -> how many documents contain this token.\n",
      " |  num_docs : int\n",
      " |      Number of documents processed.\n",
      " |  num_pos : int\n",
      " |      Total number of corpus positions (number of processed words).\n",
      " |  num_nnz : int\n",
      " |      Total number of non-zeroes in the BOW matrix (sum of the number of unique\n",
      " |      words per document over the entire corpus).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dictionary\n",
      " |      gensim.utils.SaveLoad\n",
      " |      collections.abc.Mapping\n",
      " |      collections.abc.Collection\n",
      " |      collections.abc.Sized\n",
      " |      collections.abc.Iterable\n",
      " |      collections.abc.Container\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, tokenid)\n",
      " |      Get the string token that corresponds to `tokenid`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tokenid : int\n",
      " |          Id of token.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Token corresponding to `tokenid`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If this Dictionary doesn't contain such `tokenid`.\n",
      " |  \n",
      " |  __init__(self, documents=None, prune_at=2000000)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      documents : iterable of iterable of str, optional\n",
      " |          Documents to be used to initialize the mapping and collect corpus statistics.\n",
      " |      prune_at : int, optional\n",
      " |          Dictionary will try to keep no more than `prune_at` words in its mapping, to limit its RAM\n",
      " |          footprint, the correctness is not guaranteed.\n",
      " |          Use :meth:`~gensim.corpora.dictionary.Dictionary.filter_extremes` to perform proper filtering.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>>\n",
      " |          >>> texts = [['human', 'interface', 'computer']]\n",
      " |          >>> dct = Dictionary(texts)  # initialize a Dictionary\n",
      " |          >>> dct.add_documents([[\"cat\", \"say\", \"meow\"], [\"dog\"]])  # add more document (extend the vocabulary)\n",
      " |          >>> dct.doc2bow([\"dog\", \"computer\", \"non_existent_word\"])\n",
      " |          [(0, 1), (6, 1)]\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Iterate over all tokens.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Get number of stored tokens.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          Number of stored tokens.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  add_documents(self, documents, prune_at=2000000)\n",
      " |      Update dictionary from a collection of `documents`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      documents : iterable of iterable of str\n",
      " |          Input corpus. All tokens should be already **tokenized and normalized**.\n",
      " |      prune_at : int, optional\n",
      " |          Dictionary will try to keep no more than `prune_at` words in its mapping, to limit its RAM\n",
      " |          footprint, the correctness is not guaranteed.\n",
      " |          Use :meth:`~gensim.corpora.dictionary.Dictionary.filter_extremes` to perform proper filtering.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>>\n",
      " |          >>> corpus = [\"máma mele maso\".split(), \"ema má máma\".split()]\n",
      " |          >>> dct = Dictionary(corpus)\n",
      " |          >>> len(dct)\n",
      " |          5\n",
      " |          >>> dct.add_documents([[\"this\", \"is\", \"sparta\"], [\"just\", \"joking\"]])\n",
      " |          >>> len(dct)\n",
      " |          10\n",
      " |  \n",
      " |  compactify(self)\n",
      " |      Assign new word ids to all words, shrinking any gaps.\n",
      " |  \n",
      " |  doc2bow(self, document, allow_update=False, return_missing=False)\n",
      " |      Convert `document` into the bag-of-words (BoW) format = list of `(token_id, token_count)` tuples.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      document : list of str\n",
      " |          Input document.\n",
      " |      allow_update : bool, optional\n",
      " |          Update self, by adding new tokens from `document` and updating internal corpus statistics.\n",
      " |      return_missing : bool, optional\n",
      " |          Return missing tokens (tokens present in `document` but not in self) with frequencies?\n",
      " |      \n",
      " |      Return\n",
      " |      ------\n",
      " |      list of (int, int)\n",
      " |          BoW representation of `document`.\n",
      " |      list of (int, int), dict of (str, int)\n",
      " |          If `return_missing` is True, return BoW representation of `document` + dictionary with missing\n",
      " |          tokens and their frequencies.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>> dct = Dictionary([\"máma mele maso\".split(), \"ema má máma\".split()])\n",
      " |          >>> dct.doc2bow([\"this\", \"is\", \"máma\"])\n",
      " |          [(2, 1)]\n",
      " |          >>> dct.doc2bow([\"this\", \"is\", \"máma\"], return_missing=True)\n",
      " |          ([(2, 1)], {u'this': 1, u'is': 1})\n",
      " |  \n",
      " |  doc2idx(self, document, unknown_word_index=-1)\n",
      " |      Convert `document` (a list of words) into a list of indexes = list of `token_id`.\n",
      " |      Replace all unknown words i.e, words not in the dictionary with the index as set via `unknown_word_index`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      document : list of str\n",
      " |          Input document\n",
      " |      unknown_word_index : int, optional\n",
      " |          Index to use for words not in the dictionary.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of int\n",
      " |          Token ids for tokens in `document`, in the same order.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>>\n",
      " |          >>> corpus = [[\"a\", \"a\", \"b\"], [\"a\", \"c\"]]\n",
      " |          >>> dct = Dictionary(corpus)\n",
      " |          >>> dct.doc2idx([\"a\", \"a\", \"c\", \"not_in_dictionary\", \"c\"])\n",
      " |          [0, 0, 2, -1, 2]\n",
      " |  \n",
      " |  filter_extremes(self, no_below=5, no_above=0.5, keep_n=100000, keep_tokens=None)\n",
      " |      Filter out tokens in the dictionary by their frequency.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      no_below : int, optional\n",
      " |          Keep tokens which are contained in at least `no_below` documents.\n",
      " |      no_above : float, optional\n",
      " |          Keep tokens which are contained in no more than `no_above` documents\n",
      " |          (fraction of total corpus size, not an absolute number).\n",
      " |      keep_n : int, optional\n",
      " |          Keep only the first `keep_n` most frequent tokens.\n",
      " |      keep_tokens : iterable of str\n",
      " |          Iterable of tokens that **must** stay in dictionary after filtering.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This removes all tokens in the dictionary that are:\n",
      " |      \n",
      " |      #. Less frequent than `no_below` documents (absolute number, e.g. `5`) or \n",
      " |      \n",
      " |      #. More frequent than `no_above` documents (fraction of the total corpus size, e.g. `0.3`).\n",
      " |      #. After (1) and (2), keep only the first `keep_n` most frequent tokens (or keep all if `keep_n=None`).\n",
      " |      \n",
      " |      After the pruning, resulting gaps in word ids are shrunk.\n",
      " |      Due to this gap shrinking, **the same word may have a different word id before and after the call\n",
      " |      to this function!**\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>>\n",
      " |          >>> corpus = [[\"máma\", \"mele\", \"maso\"], [\"ema\", \"má\", \"máma\"]]\n",
      " |          >>> dct = Dictionary(corpus)\n",
      " |          >>> len(dct)\n",
      " |          5\n",
      " |          >>> dct.filter_extremes(no_below=1, no_above=0.5, keep_n=1)\n",
      " |          >>> len(dct)\n",
      " |          1\n",
      " |  \n",
      " |  filter_n_most_frequent(self, remove_n)\n",
      " |      Filter out the 'remove_n' most frequent tokens that appear in the documents.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      remove_n : int\n",
      " |          Number of the most frequent tokens that will be removed.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>>\n",
      " |          >>> corpus = [[\"máma\", \"mele\", \"maso\"], [\"ema\", \"má\", \"máma\"]]\n",
      " |          >>> dct = Dictionary(corpus)\n",
      " |          >>> len(dct)\n",
      " |          5\n",
      " |          >>> dct.filter_n_most_frequent(2)\n",
      " |          >>> len(dct)\n",
      " |          3\n",
      " |  \n",
      " |  filter_tokens(self, bad_ids=None, good_ids=None)\n",
      " |      Remove the selected `bad_ids` tokens from :class:`~gensim.corpora.dictionary.Dictionary`.\n",
      " |      \n",
      " |      Alternatively, keep selected `good_ids` in :class:`~gensim.corpora.dictionary.Dictionary` and remove the rest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      bad_ids : iterable of int, optional\n",
      " |          Collection of word ids to be removed.\n",
      " |      good_ids : collection of int, optional\n",
      " |          Keep selected collection of word ids and remove the rest.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>>\n",
      " |          >>> corpus = [[\"máma\", \"mele\", \"maso\"], [\"ema\", \"má\", \"máma\"]]\n",
      " |          >>> dct = Dictionary(corpus)\n",
      " |          >>> 'ema' in dct.token2id\n",
      " |          True\n",
      " |          >>> dct.filter_tokens(bad_ids=[dct.token2id['ema']])\n",
      " |          >>> 'ema' in dct.token2id\n",
      " |          False\n",
      " |          >>> len(dct)\n",
      " |          4\n",
      " |          >>> dct.filter_tokens(good_ids=[dct.token2id['maso']])\n",
      " |          >>> len(dct)\n",
      " |          1\n",
      " |  \n",
      " |  iteritems(self)\n",
      " |  \n",
      " |  iterkeys = __iter__(self)\n",
      " |  \n",
      " |  itervalues(self)\n",
      " |  \n",
      " |  keys(self)\n",
      " |      Get all stored ids.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of int\n",
      " |          List of all token ids.\n",
      " |  \n",
      " |  merge_with(self, other)\n",
      " |      Merge another dictionary into this dictionary, mapping the same tokens to the same ids\n",
      " |      and new tokens to new ids.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The purpose is to merge two corpora created using two different dictionaries: `self` and `other`.\n",
      " |      `other` can be any id=>word mapping (a dict, a Dictionary object, ...).\n",
      " |      \n",
      " |      Return a transformation object which, when accessed as `result[doc_from_other_corpus]`, will convert documents\n",
      " |      from a corpus built using the `other` dictionary into a document using the new, merged dictionary.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : {dict, :class:`~gensim.corpora.dictionary.Dictionary`}\n",
      " |          Other dictionary.\n",
      " |      \n",
      " |      Return\n",
      " |      ------\n",
      " |      :class:`gensim.models.VocabTransform`\n",
      " |          Transformation object.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>>\n",
      " |          >>> corpus_1, corpus_2 = [[\"a\", \"b\", \"c\"]], [[\"a\", \"f\", \"f\"]]\n",
      " |          >>> dct_1, dct_2 = Dictionary(corpus_1), Dictionary(corpus_2)\n",
      " |          >>> dct_1.doc2bow(corpus_2[0])\n",
      " |          [(0, 1)]\n",
      " |          >>> transformer = dct_1.merge_with(dct_2)\n",
      " |          >>> dct_1.doc2bow(corpus_2[0])\n",
      " |          [(0, 1), (3, 2)]\n",
      " |  \n",
      " |  patch_with_special_tokens(self, special_token_dict)\n",
      " |      Patch token2id and id2token using a dictionary of special tokens.\n",
      " |      \n",
      " |      \n",
      " |      **Usecase:** when doing sequence modeling (e.g. named entity recognition), one may  want to specify\n",
      " |      special tokens that behave differently than others.\n",
      " |      One example is the \"unknown\" token, and another is the padding token.\n",
      " |      It is usual to set the padding token to have index `0`, and patching the dictionary with `{'<PAD>': 0}`\n",
      " |      would be one way to specify this.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      special_token_dict : dict of (str, int)\n",
      " |          dict containing the special tokens as keys and their wanted indices as values.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>>\n",
      " |          >>> corpus = [[\"máma\", \"mele\", \"maso\"], [\"ema\", \"má\", \"máma\"]]\n",
      " |          >>> dct = Dictionary(corpus)\n",
      " |          >>>\n",
      " |          >>> special_tokens = {'pad': 0, 'space': 1}\n",
      " |          >>> print(dct.token2id)\n",
      " |          {'maso': 0, 'mele': 1, 'máma': 2, 'ema': 3, 'má': 4}\n",
      " |          >>>\n",
      " |          >>> dct.patch_with_special_tokens(special_tokens)\n",
      " |          >>> print(dct.token2id)\n",
      " |          {'maso': 6, 'mele': 7, 'máma': 2, 'ema': 3, 'má': 4, 'pad': 0, 'space': 1}\n",
      " |  \n",
      " |  save_as_text(self, fname, sort_by_word=True)\n",
      " |      Save :class:`~gensim.corpora.dictionary.Dictionary` to a text file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to output file.\n",
      " |      sort_by_word : bool, optional\n",
      " |          Sort words in lexicographical order before writing them out?\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Format::\n",
      " |      \n",
      " |          num_docs\n",
      " |          id_1[TAB]word_1[TAB]document_frequency_1[NEWLINE]\n",
      " |          id_2[TAB]word_2[TAB]document_frequency_2[NEWLINE]\n",
      " |          ....\n",
      " |          id_k[TAB]word_k[TAB]document_frequency_k[NEWLINE]\n",
      " |      \n",
      " |      This text format is great for corpus inspection and debugging. As plaintext, it's also easily portable\n",
      " |      to other tools and frameworks. For better performance and to store the entire object state,\n",
      " |      including collected corpus statistics, use :meth:`~gensim.corpora.dictionary.Dictionary.save` and\n",
      " |      :meth:`~gensim.corpora.dictionary.Dictionary.load` instead.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.corpora.dictionary.Dictionary.load_from_text`\n",
      " |          Load :class:`~gensim.corpora.dictionary.Dictionary` from text file.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>> from gensim.test.utils import get_tmpfile\n",
      " |          >>>\n",
      " |          >>> tmp_fname = get_tmpfile(\"dictionary\")\n",
      " |          >>> corpus = [[\"máma\", \"mele\", \"maso\"], [\"ema\", \"má\", \"máma\"]]\n",
      " |          >>>\n",
      " |          >>> dct = Dictionary(corpus)\n",
      " |          >>> dct.save_as_text(tmp_fname)\n",
      " |          >>>\n",
      " |          >>> loaded_dct = Dictionary.load_from_text(tmp_fname)\n",
      " |          >>> assert dct.token2id == loaded_dct.token2id\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  from_corpus(corpus, id2word=None)\n",
      " |      Create :class:`~gensim.corpora.dictionary.Dictionary` from an existing corpus.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus : iterable of iterable of (int, number)\n",
      " |          Corpus in BoW format.\n",
      " |      id2word : dict of (int, object)\n",
      " |          Mapping id -> word. If None, the mapping `id2word[word_id] = str(word_id)` will be used.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This can be useful if you only have a term-document BOW matrix (represented by `corpus`), but not the original\n",
      " |      text corpus. This method will scan the term-document count matrix for all word ids that appear in it,\n",
      " |      then construct :class:`~gensim.corpora.dictionary.Dictionary` which maps each `word_id -> id2word[word_id]`.\n",
      " |      `id2word` is an optional dictionary that maps the `word_id` to a token.\n",
      " |      In case `id2word` isn't specified the mapping `id2word[word_id] = str(word_id)` will be used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.corpora.dictionary.Dictionary`\n",
      " |          Inferred dictionary from corpus.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>>\n",
      " |          >>> corpus = [[(1, 1.0)], [], [(0, 5.0), (2, 1.0)], []]\n",
      " |          >>> dct = Dictionary.from_corpus(corpus)\n",
      " |          >>> len(dct)\n",
      " |          3\n",
      " |  \n",
      " |  from_documents(documents)\n",
      " |      Create :class:`~gensim.corpora.dictionary.Dictionary` from `documents`.\n",
      " |      \n",
      " |      Equivalent to `Dictionary(documents=documents)`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      documents : iterable of iterable of str\n",
      " |          Input corpus.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.corpora.dictionary.Dictionary`\n",
      " |          Dictionary initialized from `documents`.\n",
      " |  \n",
      " |  load_from_text(fname)\n",
      " |      Load a previously stored :class:`~gensim.corpora.dictionary.Dictionary` from a text file.\n",
      " |      \n",
      " |      Mirror function to :meth:`~gensim.corpora.dictionary.Dictionary.save_as_text`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname: str\n",
      " |          Path to a file produced by :meth:`~gensim.corpora.dictionary.Dictionary.save_as_text`.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.corpora.dictionary.Dictionary.save_as_text`\n",
      " |          Save :class:`~gensim.corpora.dictionary.Dictionary` to text file.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.corpora import Dictionary\n",
      " |          >>> from gensim.test.utils import get_tmpfile\n",
      " |          >>>\n",
      " |          >>> tmp_fname = get_tmpfile(\"dictionary\")\n",
      " |          >>> corpus = [[\"máma\", \"mele\", \"maso\"], [\"ema\", \"má\", \"máma\"]]\n",
      " |          >>>\n",
      " |          >>> dct = Dictionary(corpus)\n",
      " |          >>> dct.save_as_text(tmp_fname)\n",
      " |          >>>\n",
      " |          >>> loaded_dct = Dictionary.load_from_text(tmp_fname)\n",
      " |          >>> assert dct.token2id == loaded_dct.token2id\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset(), pickle_protocol=2)\n",
      " |      Save the object to a file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname_or_handle : str or file-like\n",
      " |          Path to output file or already opened file-like object. If the object is a file handle,\n",
      " |          no special array handling will be performed, all attributes will be saved to the same file.\n",
      " |      separately : list of str or None, optional\n",
      " |          If None, automatically detect large numpy/scipy.sparse arrays in the object being stored, and store\n",
      " |          them into separate files. This prevent memory errors for large objects, and also allows\n",
      " |          `memory-mapping <https://en.wikipedia.org/wiki/Mmap>`_ the large arrays for efficient\n",
      " |          loading and sharing the large arrays in RAM between multiple processes.\n",
      " |      \n",
      " |          If list of str: store these attributes into separate files. The automated size check\n",
      " |          is not performed in this case.\n",
      " |      sep_limit : int, optional\n",
      " |          Don't store arrays smaller than this separately. In bytes.\n",
      " |      ignore : frozenset of str, optional\n",
      " |          Attributes that shouldn't be stored at all.\n",
      " |      pickle_protocol : int, optional\n",
      " |          Protocol number for pickle.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.utils.SaveLoad.load`\n",
      " |          Load object from file.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  load(fname, mmap=None) from abc.ABCMeta\n",
      " |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to file that contains needed object.\n",
      " |      mmap : str, optional\n",
      " |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      " |          via mmap (shared memory) using `mmap='r'.\n",
      " |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.utils.SaveLoad.save`\n",
      " |          Save object to file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      object\n",
      " |          Object loaded from `fname`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AttributeError\n",
      " |          When called on an object instance instead of class (this is a class method).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from collections.abc.Mapping:\n",
      " |  \n",
      " |  __contains__(self, key)\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  get(self, key, default=None)\n",
      " |      D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n",
      " |  \n",
      " |  items(self)\n",
      " |      D.items() -> a set-like object providing a view on D's items\n",
      " |  \n",
      " |  values(self)\n",
      " |      D.values() -> an object providing a view on D's values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from collections.abc.Mapping:\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __reversed__ = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from collections.abc.Collection:\n",
      " |  \n",
      " |  __subclasshook__(C) from abc.ABCMeta\n",
      " |      Abstract classes can override this to customize issubclass().\n",
      " |      \n",
      " |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      " |      It should return True, False or NotImplemented.  If it returns\n",
      " |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      " |      overrides the normal algorithm (and the outcome is cached).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(corpora.Dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",              \n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "# 去除通用词汇表\n",
    "stopword = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stopword]\n",
    "         for document in documents]\n",
    "\n",
    "# 去除仅出现一次的词语\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n",
      "{'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)    # 加载预料库\n",
    "dictionary.save(os.path.join(TEMP_FOLDER, 'deerwester.dict'))     # 为语料库中的所有词语分配一个整数ID\n",
    "print(dictionary)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 2)]\n"
     ]
    }
   ],
   "source": [
    "# 将文档转化为向量\n",
    "new_doc = \"Human computer interaction human\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)  # the word \"interaction\" does not appear in the dictionary and is ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用dictionary.doc2bow()方法将文档中的单词与语料中的单词进行匹配，将单词转化为语料库中的整数id,并统计次数，返回一个稀疏向量：如“human”的ID为1，文本中出现了2词则结果为：(1, 2)\n",
    "#### 其效果类似于sklearn 中的CountVectorizer类中的transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1)]\n",
      "[(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "[(2, 1), (5, 1), (7, 1), (8, 1)]\n",
      "[(1, 1), (5, 2), (8, 1)]\n",
      "[(3, 1), (6, 1), (7, 1)]\n",
      "[(9, 1)]\n",
      "[(9, 1), (10, 1)]\n",
      "[(9, 1), (10, 1), (11, 1)]\n",
      "[(4, 1), (10, 1), (11, 1)]\n"
     ]
    }
   ],
   "source": [
    "# 将原始文档转化为稀疏矩阵\n",
    "corpus = [dictionary.doc2bow(text) for text in texts] \n",
    "corpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'deerwester.mm'), corpus)    # 将结果存到磁盘\n",
    "for c in corpus:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smart_open 超大文件流读写，基于open（）方法\n",
    "from smart_open import smart_open \n",
    "class Mycorpus(object):\n",
    "    def __iter__(self):\n",
    "        for line in smart_open('test.txt', 'rb'):\n",
    "            yield dictionary.doc2bow(line.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_memeory_friendly = MyCorpus()\n",
    "print(corpus_memory_friendly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary.load(os.path.join(TEMP_FOLDER, 'deerwester.dict'))   # 读取语料库\n",
    "corpus = corpora.MmCorpus(os.path.join(TEMP_FOLDER, 'deerwester.mm'))    # d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 tfidf模型计算词语的 tfidf值\n",
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.7071067811865476), (1, 0.7071067811865476)]\n"
     ]
    }
   ],
   "source": [
    "doc_bow = [(0, 1), (1, 1)]\n",
    "print(tf[doc_bow])       # 调用模型生成向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]\n",
      "[(0, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.3244870206138555), (6, 0.44424552527467476), (7, 0.3244870206138555)]\n",
      "[(2, 0.5710059809418182), (5, 0.4170757362022777), (7, 0.4170757362022777), (8, 0.5710059809418182)]\n",
      "[(1, 0.49182558987264147), (5, 0.7184811607083769), (8, 0.49182558987264147)]\n",
      "[(3, 0.6282580468670046), (6, 0.6282580468670046), (7, 0.45889394536615247)]\n",
      "[(9, 1.0)]\n",
      "[(9, 0.7071067811865475), (10, 0.7071067811865475)]\n",
      "[(9, 0.5080429008916749), (10, 0.5080429008916749), (11, 0.695546419520037)]\n",
      "[(4, 0.6282580468670046), (10, 0.45889394536615247), (11, 0.6282580468670046)]\n"
     ]
    }
   ],
   "source": [
    "for doc in tf[corpus]:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
